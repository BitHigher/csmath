\documentclass[twocolumn]{article}

\title{Mathematics Used in Machine Learning}
\author{ZhuHanfeng}

\begin{document}

\maketitle

\begin{abstract}
Most of the math formulas below are encountered when I study machine learning,and some of them are from some books about data mining.

\end{abstract}

\section{Average}
Four widely used inequalities.

Harmonic Mean
\[\frac{n}{\frac{1}{x_1} + \frac{2}{x_2} + \frac{3}{x_3} + ... + \frac{n}{x_n}}\]

Geometric Mean
\[\sqrt[n]{x_1x_2x_3...x_n}\]

Arithmetic Mean
\[\frac{x_1 + x_2 + x_3 + ... + x_n}{n}\]

Quadratic Mean(RMS)
\[\sqrt{\frac{x_1^2 + x_2^2 + x_3^2 + ... + x_n^2}{n}}\]

\[HM \leq GM \leq AM \leq QM\]
\section{Taylor}
\[f(x) = \sum_{k=0}^n\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\]

\section{E}

\[e = \lim\limits_{\infty}(1+\frac{1}{n})^n\]

\[(1+\frac{1}{n})^n < e < (1+\frac{1}{n})^{(n+1)}\]

\[\sum\limits_{k=0}^{\infty}\frac{\lambda^k}{k!} = e^\lambda\qquad why?\]

apply Taylor
\[f(x) = e^x = \sum_{k=0}^n\frac{f^{(n)}(x_0)}{k!}{(x-x_0)^k} \]
\[x_0 = 0\]
\[f(x) = \sum_{k=0}^n\frac{1}{k!}x^k\]



\section{Normal Distribution \ Gauss Distribution}
\[X \sim N(\mu, \sigma^2)\]

\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\qquad(-\infty < x < +\infty)\]

\[F(x) = \frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^x e^{-\frac{(t-u)^2}{2\sigma^2}}d_t \]

\section{Tri functions}
\[\sin (x+y) = \sin x\cos y + \sin y\cos x\]
\[\cos (x+y) = \cos x\cos y - \sin x\sin y\]

\[(\sin x)' = \cos x\]
\[(\cos x)' = -\sin x\]

\[\sinh x = e^x - e^{-x}\]
\[\cosh x = e^x + e^{-x}\]

\section{Fourier}

\[\int_{-\pi}^{\pi} \sin nx = 0\]

\[\int_{-\pi}^{\pi} \cos nx = 0\]

\section{$\chi^2$ Distribution}
\[X \sim N(0,1)\]
\[Y = X^2\]
\[Y \sim \chi^2(1)\]
\[f_Y(y) = \frac{1}{\sqrt{2\pi}}y^{-\frac{1}{2}}e^{-\frac{y}{2}}\qquad (y>0)\]


\section{Fisher's Method}
// TODO

\section{Decision Tree}
\subsection{Gini Impurity}
\[I_G = \sum_{i=1}^k f_i(1-f_i) = \sum_{i=1}^k (f_i - f_i^2) = 1 - \sum_{i=1}^k f_i^2\]

\subsection{Entropy}
\[I_E = -\sum_{i=1}^k f_i\log_2{f_i}\]

\section{Information Theory}
Information Theory is something mainly about quantification of infomation.

\subsection{Entropy}
\[H(X) = -\sum_{x \in X} p(x)\log_2p(x)\]

\subsection{Joint Entropy}
\[H(X,Y) = -\sum_{x \in X, y \in Y} p(x,y) \log_2p(x,y)\]

\subsection{Conditional Entropy}
\[H(X|Y) = -\sum_{x \in X, y \in Y} p(x,y) \log_2 \frac{p(x,y)}{p(y)}\]
\[H(X|Y) = H(X,Y) - H(X)\]

\subsection{Mutual Information}
\[I(X;Y) = \sum_{x \in X, y \in Y}p(x,y) \log_2 \frac{p(x,y)}{p(x)p(y)}\]
\[I(X;Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)\]

\section{Matrix}
Let $A = (A_{ij})$ be an $m \times n$ matrix and Let $B = (B_{ij})$ be an $n \times p$ matrix.
Then $AB$ is an $m \times p$ matrix and
\[
	(AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}
\]

\section{Cramer's Rule}
Suppose $A$ is an $n \times n$ matrix and det(A) is not zero.It is desired to solve the system $Ax=y, y=(y_1,...,y_n)^T$ for $x = (x_1,...,x_n)^T$. Then Cramer's rule says
\[
	x_i = \frac{\det A_i}{\det A}
\]
where $A_i$ is obtained from A by replacing the $i^{th}$ column of A with $(y_1,..,y_n)^T$.

\section{Cayley Hamilton Theorem}
Let $A$ be an $n \times n$ matrix.The \emph{characteristic polynomial} is defined as
\[
	p_A(t) = det(tI - A)
\]
and the solutions to $p_A(t) = 0$ are called \emph{eigenvalues}.
\[
	p(A) = A^n + a_{n-1}A^{n-1} + ... + a_1A + a_0I
\]

\section{Radial Basis Function}
\[
	K(x_i, x_j) = e^{-\gamma \|x_i - x_j \|^2}
\]

\section{Distance / Similarity}

\subsection{Euclidean Distance}
\[
	d = \sqrt{\sum_{i=1}^n (p_i - q_i)^2}
\]

\subsection{Pearson Correlation Cofficient}
\[
	p(X, Y) = \frac{COV(X, Y)}{\sqrt{D(X) D(Y)}}
\]
\[
	COV(X, Y) = E[E(X-\overline{X})E(Y-\overline{Y})]
\]
\[
	D(X) = E[E(X-\overline{X})E(X-\overline{X})]
\]

\subsection{Tanimoto Coefficient}
The \emph{tanimoto cofficient} is a measure of the similarity of two sets. If you have two sets, A and B. 
\[
	T = \frac{count(A \cap B)}{count(A \cup B)}
\]

\section{Conditional Independent}
\[P(A_1, A_2 | V) = \frac{P(A_1, A_2, V)}{P(V)} \]
\[P(A_1, A_2 | V) = \frac{P(A_1, A_2, V)}{P(A_2, V)} \times \frac{P(A_2, V)}{P(V)} \]
\[P(A_1, A_2 | V) = P(A_1 | A_2, V) \times P(A_2 | V) \]

\section{Distribution}
\subsection{Gamma Function}
\[\Gamma (x) = \int_0^{\infty}t^{x-1}e^{-t}dt \]

\subsection{Beta Function}
\[B(m,n) = \int_0^1 x^{m-1}(1-x)^{n-1}dx  \]
\[B(m,n) = \frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)} ~~(if ~ \Gamma(n) = (n-1)!) \]


\section{Entropy}

\subsection{Kullback-Leibler divergence}
\[D_{KL}(P||Q) = \sum_x{P(x) (-\log(Q(x)) + \log(P(x)) )}\]
\[D_{KL}(P||Q) = \sum_x{P(x) \log(\frac{P(x)}{Q(x)})} \]

\subsection{Proof of Cross Entropy (Gibbs Inequality)}
\emph{P} is the true distribution and \emph{Q} is the approximation of \emph{P}.
\[H(P, Q) = -\sum_i{p_i \log q_i} \]

\begin{eqnarray*}
H(P, Q) - H(P, P) &=& -\sum{p_i \log \frac{q_i}{p_i}} \\
				& \geq & \sum{p_i (1-\frac{q_i}{p_i})} ~~(\ln x \leq x-1)\\
				& \geq & \sum{p_i} - \sum{q_i}\\
				& \geq & 1 - \sum{p_i}\\
				& \geq & 0
\end{eqnarray*}

\end{document}
